import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import random
from tqdm import tqdm
from torchvision import models

# Set random seed for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed(42)
torch.backends.cudnn.deterministic = True

# Define the CNN architecture (you can also use a pre-trained ResNet model)
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(128 * 8 * 8, 512)
        self.fc2 = nn.Linear(512, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 128 * 8 * 8)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Define a function to apply convex combination in the representation space
def convex_combine(features1, features2, alpha):
    return alpha * features1 + (1 - alpha) * features2

# Load CIFAR-10 dataset and apply transformations
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor()
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)

# Initialize the CNN model
net = CNN()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
net.to(device)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

# Training loop
epochs = 10
for epoch in range(epochs):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 200 == 199:
            print(f"[{epoch + 1}, {i + 1}] loss: {running_loss / 200:.3f}")
            running_loss = 0.0

print("Finished Training")

# Extract features from a batch of CIFAR-10 images
def extract_features(data_loader, model):
    model.eval()
    features = []
    labels = []
    with torch.no_grad():
        for inputs, targets in data_loader:
            inputs = inputs.to(device)
            features_batch = model(inputs).cpu().numpy()
            features.append(features_batch)
            labels.append(targets.cpu().numpy())
    features = np.concatenate(features)
    labels = np.concatenate(labels)
    return features, labels

train_features, train_labels = extract_features(trainloader, net)

# Apply convex combination in the representation space
alpha = 0.7  # You can adjust this value
combined_features = convex_combine(train_features[:len(train_features)//2], train_features[len(train_features)//2:], alpha)

# Apply corruptions to the combined features
corrupted_features = gaussian_noise(combined_features, severity=1)  # Example corruption function

# Define a new CNN for feature evaluation (can be the same architecture as before)
feature_net = CNN()
feature_net.to(device)

# Define loss and optimizer for feature evaluation
feature_criterion = nn.CrossEntropyLoss()
feature_optimizer = optim.SGD(feature_net.parameters(), lr=0.001, momentum=0.9)

# Convert features and labels to PyTorch tensors
corrupted_features = torch.tensor(corrupted_features).to(device)
train_labels = torch.tensor(train_labels).to(device)

# Training loop for feature evaluation
epochs = 10
for epoch in range(epochs):
    running_loss = 0.0
    for i in range(0, len(corrupted_features), 64):
        inputs = corrupted_features[i:i+64]
        labels = train_labels[i:i+64]

        feature_optimizer.zero_grad()

        # Forward pass
        outputs = feature_net(inputs)
        loss = feature_criterion(outputs, labels)
        loss.backward()
        feature_optimizer.step()

        running_loss += loss.item()

    print(f"[{epoch + 1}] loss: {running_loss / (len(corrupted_features) / 64):.3f}")

print("Finished Feature Training")

# Evaluate the feature evaluation network on clean or corrupted test data
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)

def evaluate(model, data_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data in data_loader:
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    return accuracy

# Evaluate on clean test data
clean_accuracy = evaluate(feature_net, testloader)
print(f"Accuracy on clean test data: {clean_accuracy:.2f}%")

# Evaluate on corrupted test data (you can choose the severity level)
corrupted_features_test = gaussian_noise(train_features, severity=1)  # Example corruption function
corrupted_features_test = torch.tensor(corrupted_features_test).to(device)

corrupted_accuracy = evaluate(feature_net, testloader)
print(f"Accuracy on corrupted test data: {corrupted_accuracy:.2f}%")
